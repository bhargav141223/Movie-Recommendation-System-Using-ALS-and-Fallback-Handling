{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QQu8SD9t2WvN",
    "outputId": "f0319f7b-6d3d-4c53-86d1-8a283632c617"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dbutils\n",
      "  Downloading dbutils-3.1.2-py3-none-any.whl.metadata (2.4 kB)\n",
      "Downloading dbutils-3.1.2-py3-none-any.whl (32 kB)\n",
      "Installing collected packages: dbutils\n",
      "Successfully installed dbutils-3.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install dbutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7469847a",
    "outputId": "6f4c5f74-7024-4d92-e0b9-c5f876fb0861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlflow\n",
      "  Downloading mlflow-3.6.0-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting mlflow-skinny==3.6.0 (from mlflow)\n",
      "  Downloading mlflow_skinny-3.6.0-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting mlflow-tracing==3.6.0 (from mlflow)\n",
      "  Downloading mlflow_tracing-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting Flask-CORS<7 (from mlflow)\n",
      "  Downloading flask_cors-6.0.1-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.1.2)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.12/dist-packages (from mlflow) (1.17.1)\n",
      "Requirement already satisfied: cryptography<47,>=43.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (43.0.3)\n",
      "Collecting docker<8,>=4.0.0 (from mlflow)\n",
      "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting graphene<4 (from mlflow)\n",
      "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting gunicorn<24 (from mlflow)\n",
      "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting huey<3,>=2.5.0 (from mlflow)\n",
      "  Downloading huey-2.5.4-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.10.0)\n",
      "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.0.2)\n",
      "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.2.2)\n",
      "Requirement already satisfied: pyarrow<23,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (18.1.0)\n",
      "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.12/dist-packages (from mlflow) (1.6.1)\n",
      "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.12/dist-packages (from mlflow) (1.16.3)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.0.44)\n",
      "Requirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (5.5.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (8.3.0)\n",
      "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (3.1.2)\n",
      "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==3.6.0->mlflow)\n",
      "  Downloading databricks_sdk-0.73.0-py3-none-any.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fastapi<1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (0.121.0)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (3.1.45)\n",
      "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (8.7.0)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-proto<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (1.37.0)\n",
      "Requirement already satisfied: packaging<26 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (25.0)\n",
      "Requirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (5.29.5)\n",
      "Requirement already satisfied: pydantic<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (2.11.10)\n",
      "Requirement already satisfied: python-dotenv<2,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (1.2.1)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (6.0.3)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (2.32.4)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (4.15.0)\n",
      "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (0.38.0)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.3.10)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography<47,>=43.0.0->mlflow) (2.0.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from docker<8,>=4.0.0->mlflow) (2.5.0)\n",
      "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
      "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
      "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.1.6)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.0.3)\n",
      "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
      "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
      "  Downloading graphql_core-3.2.7-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
      "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from graphene<4->mlflow) (2.9.0.post0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (3.2.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3->mlflow) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3->mlflow) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2->mlflow) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2->mlflow) (3.6.0)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.2.4)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography<47,>=43.0.0->mlflow) (2.23)\n",
      "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.12/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.6.0->mlflow) (2.38.0)\n",
      "Requirement already satisfied: starlette<0.50.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi<1->mlflow-skinny==3.6.0->mlflow) (0.49.3)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1->mlflow-skinny==3.6.0->mlflow) (0.0.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.6.0->mlflow) (4.0.12)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.6.0->mlflow) (3.23.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.6.0->mlflow) (0.58b0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.6.0->mlflow) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.6.0->mlflow) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.6.0->mlflow) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.6.0->mlflow) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.6.0->mlflow) (3.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.6.0->mlflow) (2025.10.5)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn<1->mlflow-skinny==3.6.0->mlflow) (0.16.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.6.0->mlflow) (5.0.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.6.0->mlflow) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.6.0->mlflow) (4.9.1)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.50.0,>=0.40.0->fastapi<1->mlflow-skinny==3.6.0->mlflow) (4.11.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette<0.50.0,>=0.40.0->fastapi<1->mlflow-skinny==3.6.0->mlflow) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.6.0->mlflow) (0.6.1)\n",
      "Downloading mlflow-3.6.0-py3-none-any.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mlflow_skinny-3.6.0-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mlflow_tracing-3.6.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading flask_cors-6.0.1-py3-none-any.whl (13 kB)\n",
      "Downloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huey-2.5.4-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.8/76.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading databricks_sdk-0.73.0-py3-none-any.whl (753 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m753.9/753.9 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading graphql_core-3.2.7-py3-none-any.whl (207 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: huey, gunicorn, graphql-core, graphql-relay, docker, graphene, Flask-CORS, databricks-sdk, mlflow-tracing, mlflow-skinny, mlflow\n",
      "Successfully installed Flask-CORS-6.0.1 databricks-sdk-0.73.0 docker-7.1.0 graphene-3.4.3 graphql-core-3.2.7 graphql-relay-3.2.0 gunicorn-23.0.0 huey-2.5.4 mlflow-3.6.0 mlflow-skinny-3.6.0 mlflow-tracing-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fAraZ99q1yj6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "%matplotlib inline\n",
    "\n",
    "# dbutils.library.installPyPI(\"mlflow\")\n",
    "# dbutils.library.restartPython()\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AgSi5MrY3Ame",
    "outputId": "90b35f31-d9f9-45ce-c181-4756051fea01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  ca-certificates-java fonts-dejavu-core fonts-dejavu-extra java-common\n",
      "  libatk-wrapper-java libatk-wrapper-java-jni libpcsclite1 libxt-dev libxtst6\n",
      "  libxxf86dga1 openjdk-11-jdk-headless openjdk-11-jre openjdk-11-jre-headless\n",
      "  x11-utils\n",
      "Suggested packages:\n",
      "  default-jre pcscd libxt-doc openjdk-11-demo openjdk-11-source visualvm\n",
      "  libnss-mdns fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
      "  | fonts-wqy-zenhei fonts-indic mesa-utils\n",
      "The following NEW packages will be installed:\n",
      "  ca-certificates-java fonts-dejavu-core fonts-dejavu-extra java-common\n",
      "  libatk-wrapper-java libatk-wrapper-java-jni libpcsclite1 libxt-dev libxtst6\n",
      "  libxxf86dga1 openjdk-11-jdk openjdk-11-jdk-headless openjdk-11-jre\n",
      "  openjdk-11-jre-headless x11-utils\n",
      "0 upgraded, 15 newly installed, 0 to remove and 41 not upgraded.\n",
      "Need to get 122 MB of archives.\n",
      "After this operation, 274 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 java-common all 0.72build2 [6,782 B]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpcsclite1 amd64 1.9.5-3ubuntu1 [19.8 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre-headless amd64 11.0.28+6-1ubuntu1~22.04.1 [42.6 MB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ca-certificates-java all 20190909ubuntu1.2 [12.1 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre amd64 11.0.28+6-1ubuntu1~22.04.1 [214 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jdk-headless amd64 11.0.28+6-1ubuntu1~22.04.1 [73.6 MB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jdk amd64 11.0.28+6-1ubuntu1~22.04.1 [1,342 kB]\n",
      "Fetched 122 MB in 2s (72.5 MB/s)\n",
      "Selecting previously unselected package java-common.\n",
      "(Reading database ... 125082 files and directories currently installed.)\n",
      "Preparing to unpack .../00-java-common_0.72build2_all.deb ...\n",
      "Unpacking java-common (0.72build2) ...\n",
      "Selecting previously unselected package libpcsclite1:amd64.\n",
      "Preparing to unpack .../01-libpcsclite1_1.9.5-3ubuntu1_amd64.deb ...\n",
      "Unpacking libpcsclite1:amd64 (1.9.5-3ubuntu1) ...\n",
      "Selecting previously unselected package openjdk-11-jre-headless:amd64.\n",
      "Preparing to unpack .../02-openjdk-11-jre-headless_11.0.28+6-1ubuntu1~22.04.1_amd64.deb ...\n",
      "Unpacking openjdk-11-jre-headless:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
      "Selecting previously unselected package ca-certificates-java.\n",
      "Preparing to unpack .../03-ca-certificates-java_20190909ubuntu1.2_all.deb ...\n",
      "Unpacking ca-certificates-java (20190909ubuntu1.2) ...\n",
      "Selecting previously unselected package fonts-dejavu-core.\n",
      "Preparing to unpack .../04-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
      "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
      "Selecting previously unselected package fonts-dejavu-extra.\n",
      "Preparing to unpack .../05-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
      "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
      "Selecting previously unselected package libxtst6:amd64.\n",
      "Preparing to unpack .../06-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
      "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
      "Selecting previously unselected package libxxf86dga1:amd64.\n",
      "Preparing to unpack .../07-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
      "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
      "Selecting previously unselected package x11-utils.\n",
      "Preparing to unpack .../08-x11-utils_7.7+5build2_amd64.deb ...\n",
      "Unpacking x11-utils (7.7+5build2) ...\n",
      "Selecting previously unselected package libatk-wrapper-java.\n",
      "Preparing to unpack .../09-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
      "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
      "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
      "Preparing to unpack .../10-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
      "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
      "Selecting previously unselected package libxt-dev:amd64.\n",
      "Preparing to unpack .../11-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n",
      "Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n",
      "Selecting previously unselected package openjdk-11-jre:amd64.\n",
      "Preparing to unpack .../12-openjdk-11-jre_11.0.28+6-1ubuntu1~22.04.1_amd64.deb ...\n",
      "Unpacking openjdk-11-jre:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
      "Selecting previously unselected package openjdk-11-jdk-headless:amd64.\n",
      "Preparing to unpack .../13-openjdk-11-jdk-headless_11.0.28+6-1ubuntu1~22.04.1_amd64.deb ...\n",
      "Unpacking openjdk-11-jdk-headless:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
      "Selecting previously unselected package openjdk-11-jdk:amd64.\n",
      "Preparing to unpack .../14-openjdk-11-jdk_11.0.28+6-1ubuntu1~22.04.1_amd64.deb ...\n",
      "Unpacking openjdk-11-jdk:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
      "Setting up java-common (0.72build2) ...\n",
      "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
      "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
      "Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n",
      "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
      "Setting up libpcsclite1:amd64 (1.9.5-3ubuntu1) ...\n",
      "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
      "Setting up x11-utils (7.7+5build2) ...\n",
      "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
      "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
      "Setting up openjdk-11-jre-headless:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/java to provide /usr/bin/java (java) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jjs to provide /usr/bin/jjs (jjs) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/keytool to provide /usr/bin/keytool (keytool) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/rmid to provide /usr/bin/rmid (rmid) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/rmiregistry to provide /usr/bin/rmiregistry (rmiregistry) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/pack200 to provide /usr/bin/pack200 (pack200) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/unpack200 to provide /usr/bin/unpack200 (unpack200) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/lib/jexec to provide /usr/bin/jexec (jexec) in auto mode\n",
      "Setting up openjdk-11-jre:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
      "Setting up openjdk-11-jdk-headless:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jar to provide /usr/bin/jar (jar) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jarsigner to provide /usr/bin/jarsigner (jarsigner) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/javac to provide /usr/bin/javac (javac) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/javadoc to provide /usr/bin/javadoc (javadoc) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/javap to provide /usr/bin/javap (javap) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jcmd to provide /usr/bin/jcmd (jcmd) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jdb to provide /usr/bin/jdb (jdb) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jdeprscan to provide /usr/bin/jdeprscan (jdeprscan) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jdeps to provide /usr/bin/jdeps (jdeps) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jfr to provide /usr/bin/jfr (jfr) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jimage to provide /usr/bin/jimage (jimage) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jinfo to provide /usr/bin/jinfo (jinfo) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jlink to provide /usr/bin/jlink (jlink) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jmap to provide /usr/bin/jmap (jmap) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jmod to provide /usr/bin/jmod (jmod) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jps to provide /usr/bin/jps (jps) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jrunscript to provide /usr/bin/jrunscript (jrunscript) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jshell to provide /usr/bin/jshell (jshell) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jstack to provide /usr/bin/jstack (jstack) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jstat to provide /usr/bin/jstat (jstat) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jstatd to provide /usr/bin/jstatd (jstatd) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/rmic to provide /usr/bin/rmic (rmic) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/serialver to provide /usr/bin/serialver (serialver) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jaotc to provide /usr/bin/jaotc (jaotc) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jhsdb to provide /usr/bin/jhsdb (jhsdb) in auto mode\n",
      "Setting up openjdk-11-jdk:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jconsole to provide /usr/bin/jconsole (jconsole) in auto mode\n",
      "Setting up ca-certificates-java (20190909ubuntu1.2) ...\n",
      "head: cannot open '/etc/ssl/certs/java/cacerts' for reading: No such file or directory\n",
      "Adding debian:SecureSign_RootCA11.pem\n",
      "Adding debian:USERTrust_RSA_Certification_Authority.pem\n",
      "Adding debian:AffirmTrust_Commercial.pem\n",
      "Adding debian:DigiCert_Global_Root_G3.pem\n",
      "Adding debian:Entrust_Root_Certification_Authority_-_G2.pem\n",
      "Adding debian:certSIGN_Root_CA_G2.pem\n",
      "Adding debian:Starfield_Root_Certificate_Authority_-_G2.pem\n",
      "Adding debian:SSL.com_EV_Root_Certification_Authority_RSA_R2.pem\n",
      "Adding debian:QuoVadis_Root_CA_3.pem\n",
      "Adding debian:GlobalSign_Root_CA_-_R6.pem\n",
      "Adding debian:D-TRUST_EV_Root_CA_1_2020.pem\n",
      "Adding debian:HARICA_TLS_RSA_Root_CA_2021.pem\n",
      "Adding debian:Trustwave_Global_Certification_Authority.pem\n",
      "Adding debian:QuoVadis_Root_CA_2_G3.pem\n",
      "Adding debian:Comodo_AAA_Services_root.pem\n",
      "Adding debian:Certum_EC-384_CA.pem\n",
      "Adding debian:GlobalSign_ECC_Root_CA_-_R4.pem\n",
      "Adding debian:Hellenic_Academic_and_Research_Institutions_ECC_RootCA_2015.pem\n",
      "Adding debian:GlobalSign_Root_CA.pem\n",
      "Adding debian:HiPKI_Root_CA_-_G1.pem\n",
      "Adding debian:emSign_Root_CA_-_G1.pem\n",
      "Adding debian:SSL.com_Root_Certification_Authority_RSA.pem\n",
      "Adding debian:CFCA_EV_ROOT.pem\n",
      "Adding debian:COMODO_Certification_Authority.pem\n",
      "Adding debian:DigiCert_High_Assurance_EV_Root_CA.pem\n",
      "Adding debian:T-TeleSec_GlobalRoot_Class_3.pem\n",
      "Adding debian:D-TRUST_Root_Class_3_CA_2_2009.pem\n",
      "Adding debian:Autoridad_de_Certificacion_Firmaprofesional_CIF_A62634068.pem\n",
      "Adding debian:Izenpe.com.pem\n",
      "Adding debian:HARICA_TLS_ECC_Root_CA_2021.pem\n",
      "Adding debian:D-TRUST_Root_Class_3_CA_2_EV_2009.pem\n",
      "Adding debian:Security_Communication_RootCA2.pem\n",
      "Adding debian:UCA_Global_G2_Root.pem\n",
      "Adding debian:D-TRUST_BR_Root_CA_1_2020.pem\n",
      "Adding debian:Secure_Global_CA.pem\n",
      "Adding debian:GTS_Root_R3.pem\n",
      "Adding debian:ISRG_Root_X1.pem\n",
      "Adding debian:Entrust_Root_Certification_Authority.pem\n",
      "Adding debian:Hellenic_Academic_and_Research_Institutions_RootCA_2015.pem\n",
      "Adding debian:GlobalSign_Root_E46.pem\n",
      "Adding debian:vTrus_Root_CA.pem\n",
      "Adding debian:TWCA_Root_Certification_Authority.pem\n",
      "Adding debian:AffirmTrust_Premium.pem\n",
      "Adding debian:XRamp_Global_CA_Root.pem\n",
      "Adding debian:Starfield_Class_2_CA.pem\n",
      "Adding debian:Buypass_Class_2_Root_CA.pem\n",
      "Adding debian:Entrust.net_Premium_2048_Secure_Server_CA.pem\n",
      "Adding debian:SSL.com_EV_Root_Certification_Authority_ECC.pem\n",
      "Adding debian:Starfield_Services_Root_Certificate_Authority_-_G2.pem\n",
      "Adding debian:Amazon_Root_CA_2.pem\n",
      "Adding debian:GLOBALTRUST_2020.pem\n",
      "Adding debian:Microsoft_ECC_Root_Certificate_Authority_2017.pem\n",
      "Adding debian:certSIGN_ROOT_CA.pem\n",
      "Adding debian:OISTE_WISeKey_Global_Root_GC_CA.pem\n",
      "Adding debian:DigiCert_Assured_ID_Root_G2.pem\n",
      "Adding debian:OISTE_WISeKey_Global_Root_GB_CA.pem\n",
      "Adding debian:ePKI_Root_Certification_Authority.pem\n",
      "Adding debian:Certum_Trusted_Root_CA.pem\n",
      "Adding debian:Security_Communication_ECC_RootCA1.pem\n",
      "Adding debian:Amazon_Root_CA_1.pem\n",
      "Adding debian:ACCVRAIZ1.pem\n",
      "Adding debian:QuoVadis_Root_CA_2.pem\n",
      "Adding debian:TWCA_Global_Root_CA.pem\n",
      "Adding debian:Amazon_Root_CA_3.pem\n",
      "Adding debian:emSign_Root_CA_-_C1.pem\n",
      "Adding debian:DigiCert_Global_Root_CA.pem\n",
      "Adding debian:Security_Communication_RootCA3.pem\n",
      "Adding debian:UCA_Extended_Validation_Root.pem\n",
      "Adding debian:GTS_Root_R1.pem\n",
      "Adding debian:Baltimore_CyberTrust_Root.pem\n",
      "Adding debian:GDCA_TrustAUTH_R5_ROOT.pem\n",
      "Adding debian:Certum_Trusted_Network_CA_2.pem\n",
      "Adding debian:Microsec_e-Szigno_Root_CA_2009.pem\n",
      "Adding debian:NAVER_Global_Root_Certification_Authority.pem\n",
      "Adding debian:GTS_Root_R4.pem\n",
      "Adding debian:Go_Daddy_Root_Certificate_Authority_-_G2.pem\n",
      "Adding debian:Buypass_Class_3_Root_CA.pem\n",
      "Adding debian:e-Szigno_Root_CA_2017.pem\n",
      "Adding debian:Telia_Root_CA_v2.pem\n",
      "Adding debian:QuoVadis_Root_CA_1_G3.pem\n",
      "Adding debian:Certainly_Root_E1.pem\n",
      "Adding debian:AC_RAIZ_FNMT-RCM.pem\n",
      "Adding debian:DigiCert_TLS_ECC_P384_Root_G5.pem\n",
      "Adding debian:AffirmTrust_Networking.pem\n",
      "Adding debian:COMODO_RSA_Certification_Authority.pem\n",
      "Adding debian:GlobalSign_Root_R46.pem\n",
      "Adding debian:Trustwave_Global_ECC_P384_Certification_Authority.pem\n",
      "Adding debian:TUBITAK_Kamu_SM_SSL_Kok_Sertifikasi_-_Surum_1.pem\n",
      "Adding debian:Go_Daddy_Class_2_CA.pem\n",
      "Adding debian:Certigna_Root_CA.pem\n",
      "Adding debian:vTrus_ECC_Root_CA.pem\n",
      "Adding debian:GlobalSign_ECC_Root_CA_-_R5.pem\n",
      "Adding debian:NetLock_Arany_=Class_Gold=_Főtanúsítvány.pem\n",
      "Adding debian:Microsoft_RSA_Root_Certificate_Authority_2017.pem\n",
      "Adding debian:SZAFIR_ROOT_CA2.pem\n",
      "Adding debian:Certum_Trusted_Network_CA.pem\n",
      "Adding debian:CA_Disig_Root_R2.pem\n",
      "Adding debian:Trustwave_Global_ECC_P256_Certification_Authority.pem\n",
      "Adding debian:Hongkong_Post_Root_CA_3.pem\n",
      "Adding debian:QuoVadis_Root_CA_3_G3.pem\n",
      "Adding debian:SSL.com_Root_Certification_Authority_ECC.pem\n",
      "Adding debian:Entrust_Root_Certification_Authority_-_G4.pem\n",
      "Adding debian:GTS_Root_R2.pem\n",
      "Adding debian:ISRG_Root_X2.pem\n",
      "Adding debian:emSign_ECC_Root_CA_-_C3.pem\n",
      "Adding debian:SwissSign_Silver_CA_-_G2.pem\n",
      "Adding debian:Actalis_Authentication_Root_CA.pem\n",
      "Adding debian:T-TeleSec_GlobalRoot_Class_2.pem\n",
      "Adding debian:ANF_Secure_Server_Root_CA.pem\n",
      "Adding debian:USERTrust_ECC_Certification_Authority.pem\n",
      "Adding debian:COMODO_ECC_Certification_Authority.pem\n",
      "Adding debian:DigiCert_Global_Root_G2.pem\n",
      "Adding debian:Security_Communication_Root_CA.pem\n",
      "Adding debian:AC_RAIZ_FNMT-RCM_SERVIDORES_SEGUROS.pem\n",
      "Adding debian:DigiCert_TLS_RSA4096_Root_G5.pem\n",
      "Adding debian:DigiCert_Assured_ID_Root_G3.pem\n",
      "Adding debian:TeliaSonera_Root_CA_v1.pem\n",
      "Adding debian:SecureTrust_CA.pem\n",
      "Adding debian:DigiCert_Trusted_Root_G4.pem\n",
      "Adding debian:Certainly_Root_R1.pem\n",
      "Adding debian:Entrust_Root_Certification_Authority_-_EC1.pem\n",
      "Adding debian:TunTrust_Root_CA.pem\n",
      "Adding debian:IdenTrust_Commercial_Root_CA_1.pem\n",
      "Adding debian:Certigna.pem\n",
      "Adding debian:Amazon_Root_CA_4.pem\n",
      "Adding debian:SwissSign_Gold_CA_-_G2.pem\n",
      "Adding debian:DigiCert_Assured_ID_Root_CA.pem\n",
      "Adding debian:AffirmTrust_Premium_ECC.pem\n",
      "Adding debian:Atos_TrustedRoot_2011.pem\n",
      "Adding debian:GlobalSign_Root_CA_-_R3.pem\n",
      "Adding debian:IdenTrust_Public_Sector_Root_CA_1.pem\n",
      "Adding debian:emSign_ECC_Root_CA_-_G3.pem\n",
      "Adding debian:Sectigo_Public_Server_Authentication_Root_R46.pem\n",
      "Adding debian:Atos_TrustedRoot_Root_CA_ECC_TLS_2021.pem\n",
      "Adding debian:Atos_TrustedRoot_Root_CA_RSA_TLS_2021.pem\n",
      "Adding debian:BJCA_Global_Root_CA2.pem\n",
      "Adding debian:BJCA_Global_Root_CA1.pem\n",
      "Adding debian:CommScope_Public_Trust_ECC_Root-01.pem\n",
      "Adding debian:Sectigo_Public_Server_Authentication_Root_E46.pem\n",
      "Adding debian:SSL.com_TLS_ECC_Root_CA_2022.pem\n",
      "Adding debian:SSL.com_TLS_RSA_Root_CA_2022.pem\n",
      "Adding debian:TrustAsia_Global_Root_CA_G4.pem\n",
      "Adding debian:CommScope_Public_Trust_RSA_Root-01.pem\n",
      "Adding debian:CommScope_Public_Trust_RSA_Root-02.pem\n",
      "Adding debian:TrustAsia_Global_Root_CA_G3.pem\n",
      "Adding debian:CommScope_Public_Trust_ECC_Root-02.pem\n",
      "done.\n",
      "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
      "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
      "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
      "\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      "Processing triggers for ca-certificates (20240203~22.04.1) ...\n",
      "Updating certificates in /etc/ssl/certs...\n",
      "0 added, 0 removed; done.\n",
      "Running hooks in /etc/ca-certificates/update.d...\n",
      "\n",
      "done.\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "!apt-get install openjdk-11-jdk -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "eIvY6oxp3Fm5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] += \":/usr/lib/jvm/java-11-openjdk-amd64/bin\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "goSom_iF31xK",
    "outputId": "bf8636e0-ef7b-4b55-f0e6-0b04faa9ad43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "fnFKHDI84Fhu"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Yj_diQXm2Eus"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"moive analysis\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ZuseRbGH4H1s"
   },
   "outputs": [],
   "source": [
    "movies_df = spark.read.load(\"/content/movies.csv\", format='csv', header = True)\n",
    "ratings_df = spark.read.load(\"/content/ratings.csv\", format='csv', header = True)\n",
    "links_df = spark.read.load(\"/content/links.csv\", format='csv', header = True)\n",
    "tags_df = spark.read.load(\"/content/tags.csv\", format='csv', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "30L1QEto4I_2",
    "outputId": "403fc0a7-a203-4f58-aa29-23720c58df2e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
       "      pre.function-repr-contents {\n",
       "        overflow-x: auto;\n",
       "        padding: 8px 12px;\n",
       "        max-height: 500px;\n",
       "      }\n",
       "\n",
       "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
       "        cursor: pointer;\n",
       "        max-height: 100px;\n",
       "      }\n",
       "    </style>\n",
       "    <pre style=\"white-space: initial; background:\n",
       "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
       "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.sql.dataframe.DataFrame</b><br/>def __init__(jdf: JavaObject, sql_ctx: Union[&#x27;SQLContext&#x27;, &#x27;SparkSession&#x27;])</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/pyspark/sql/dataframe.py</a>A distributed collection of data grouped into named columns.\n",
       "\n",
       ".. versionadded:: 1.3.0\n",
       "\n",
       ".. versionchanged:: 3.4.0\n",
       "    Supports Spark Connect.\n",
       "\n",
       "Examples\n",
       "--------\n",
       "A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n",
       "and can be created using various functions in :class:`SparkSession`:\n",
       "\n",
       "&gt;&gt;&gt; people = spark.createDataFrame([\n",
       "...     {&quot;deptId&quot;: 1, &quot;age&quot;: 40, &quot;name&quot;: &quot;Hyukjin Kwon&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 50},\n",
       "...     {&quot;deptId&quot;: 1, &quot;age&quot;: 50, &quot;name&quot;: &quot;Takuya Ueshin&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 100},\n",
       "...     {&quot;deptId&quot;: 2, &quot;age&quot;: 60, &quot;name&quot;: &quot;Xinrong Meng&quot;, &quot;gender&quot;: &quot;F&quot;, &quot;salary&quot;: 150},\n",
       "...     {&quot;deptId&quot;: 3, &quot;age&quot;: 20, &quot;name&quot;: &quot;Haejoon Lee&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 200}\n",
       "... ])\n",
       "\n",
       "Once created, it can be manipulated using the various domain-specific-language\n",
       "(DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n",
       "\n",
       "To select a column from the :class:`DataFrame`, use the apply method:\n",
       "\n",
       "&gt;&gt;&gt; age_col = people.age\n",
       "\n",
       "A more concrete example:\n",
       "\n",
       "&gt;&gt;&gt; # To create DataFrame using SparkSession\n",
       "... department = spark.createDataFrame([\n",
       "...     {&quot;id&quot;: 1, &quot;name&quot;: &quot;PySpark&quot;},\n",
       "...     {&quot;id&quot;: 2, &quot;name&quot;: &quot;ML&quot;},\n",
       "...     {&quot;id&quot;: 3, &quot;name&quot;: &quot;Spark SQL&quot;}\n",
       "... ])\n",
       "\n",
       "&gt;&gt;&gt; people.filter(people.age &gt; 30).join(\n",
       "...     department, people.deptId == department.id).groupBy(\n",
       "...     department.name, &quot;gender&quot;).agg({&quot;salary&quot;: &quot;avg&quot;, &quot;age&quot;: &quot;max&quot;}).show()\n",
       "+-------+------+-----------+--------+\n",
       "|   name|gender|avg(salary)|max(age)|\n",
       "+-------+------+-----------+--------+\n",
       "|     ML|     F|      150.0|      60|\n",
       "|PySpark|     M|       75.0|      50|\n",
       "+-------+------+-----------+--------+\n",
       "\n",
       "Notes\n",
       "-----\n",
       "A DataFrame should only be created as described above. It should not be directly\n",
       "created via using the constructor.</pre>\n",
       "      <script>\n",
       "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
       "        for (const element of document.querySelectorAll('.filepath')) {\n",
       "          element.style.display = 'block'\n",
       "          element.onclick = (event) => {\n",
       "            event.preventDefault();\n",
       "            event.stopPropagation();\n",
       "            google.colab.files.view(element.textContent, 80);\n",
       "          };\n",
       "        }\n",
       "      }\n",
       "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
       "        element.onclick = (event) => {\n",
       "          event.preventDefault();\n",
       "          event.stopPropagation();\n",
       "          element.classList.toggle('function-repr-contents-collapsed');\n",
       "        };\n",
       "      }\n",
       "      </script>\n",
       "      </div>"
      ],
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(movies_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J2SzU9Kd4Yqb",
    "outputId": "7e915296-6d86-48e8-fe6f-4136dbce7a91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87585"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pMAXeqoW4cbB",
    "outputId": "33612f63-1e6f-4395-b602-0014c8a58c31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|movieId|               title|              genres|\n",
      "+-------+--------------------+--------------------+\n",
      "|      1|    Toy Story (1995)|Adventure|Animati...|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n",
      "|      5|Father of the Bri...|              Comedy|\n",
      "|      6|         Heat (1995)|Action|Crime|Thri...|\n",
      "|      7|      Sabrina (1995)|      Comedy|Romance|\n",
      "|      8| Tom and Huck (1995)|  Adventure|Children|\n",
      "|      9| Sudden Death (1995)|              Action|\n",
      "|     10|    GoldenEye (1995)|Action|Adventure|...|\n",
      "+-------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_df.createOrReplaceTempView(\"movies_df\")\n",
    "\n",
    "spark.sql(\"SELECT movieId, title, genres FROM movies_df LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ITKSuTJ4fuL",
    "outputId": "ac910c73-709e-4480-c138-78539f7f20b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     1|     17|   4.0|944249077|\n",
      "|     1|     25|   1.0|944250228|\n",
      "|     1|     29|   2.0|943230976|\n",
      "|     1|     30|   5.0|944249077|\n",
      "|     1|     32|   5.0|943228858|\n",
      "|     1|     34|   2.0|943228491|\n",
      "|     1|     36|   1.0|944249008|\n",
      "|     1|     80|   5.0|944248943|\n",
      "|     1|    110|   3.0|943231119|\n",
      "|     1|    111|   5.0|944249008|\n",
      "+------+-------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ratings_df.show(10)\n",
    "ratings_df.createOrReplaceTempView(\"ratings_df\")\n",
    "spark.sql(\"SELECT * FROM ratings_df LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2wmxXDJo4hYJ",
    "outputId": "ca79ee77-8ed9-4adb-c52b-07783b3bc23f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+\n",
      "|movieId| imdbId|tmdbId|\n",
      "+-------+-------+------+\n",
      "|      1|0114709|   862|\n",
      "|      2|0113497|  8844|\n",
      "|      3|0113228| 15602|\n",
      "|      4|0114885| 31357|\n",
      "|      5|0113041| 11862|\n",
      "|      6|0113277|   949|\n",
      "|      7|0114319| 11860|\n",
      "|      8|0112302| 45325|\n",
      "|      9|0114576|  9091|\n",
      "|     10|0113189|   710|\n",
      "+-------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# For Links Dataset\n",
    "links_df.createOrReplaceTempView(\"links_df\")\n",
    "spark.sql(\"SELECT * FROM links_df LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d_sz7-KW4jjh",
    "outputId": "a6c3368a-c29d-42f0-f131-7c6c78e9f0a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------------------+----------+\n",
      "|userId|movieId|                 tag| timestamp|\n",
      "+------+-------+--------------------+----------+\n",
      "|    22|  26479|         Kevin Kline|1583038886|\n",
      "|    22|  79592|            misogyny|1581476297|\n",
      "|    22| 247150|          acrophobia|1622483469|\n",
      "|    34|   2174|               music|1249808064|\n",
      "|    34|   2174|               weird|1249808102|\n",
      "|    34|   8623|        Steve Martin|1249808497|\n",
      "|    55|   5766|the killls and th...|1319322078|\n",
      "|    58|   7451|            bullying|1672551536|\n",
      "|    58|   7451|              clique|1672551510|\n",
      "|    58|   7451|       coming of age|1672551502|\n",
      "+------+-------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For Tags Dataset\n",
    "tags_df.createOrReplaceTempView(\"tags_df\")\n",
    "spark.sql(\"SELECT * FROM tags_df LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "93Fjwwfw4nAo",
    "outputId": "a6583091-32b2-4449-b794-7fda4ff1b199"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the users that rated movies and the movies that were rated:\n",
      "Minimum number of ratings per user is 20\n",
      "Minimum number of ratings per movie is 1\n"
     ]
    }
   ],
   "source": [
    "tmp1 = ratings_df.groupBy(\"userID\").count().toPandas()['count'].min()\n",
    "tmp2 = ratings_df.groupBy(\"movieId\").count().toPandas()['count'].min()\n",
    "print('For the users that rated movies and the movies that were rated:')\n",
    "print('Minimum number of ratings per user is {}'.format(tmp1))\n",
    "print('Minimum number of ratings per movie is {}'.format(tmp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xh7OTjDR4xxR",
    "outputId": "7c63a84d-6a2e-4c61-f670-ad620d666d81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13716 out of 38784 movies are rated by only one user\n"
     ]
    }
   ],
   "source": [
    "tmp1 = sum(ratings_df.groupBy(\"movieId\").count().toPandas()['count'] == 1)\n",
    "tmp2 = ratings_df.select('movieId').distinct().count()\n",
    "print('{} out of {} movies are rated by only one user'.format(tmp1, tmp2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqAVG-q543Hz"
   },
   "source": [
    "\n",
    "**Part 1: Spark SQL and OLAP**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ZFJqjod40e4",
    "outputId": "d15eb56a-2f3b-4c99-8914-09b7a98197e7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pyspark/sql/dataframe.py:329: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "movies_df.registerTempTable(\"movies\")\n",
    "ratings_df.registerTempTable(\"ratings\")\n",
    "links_df.registerTempTable(\"links\")\n",
    "tags_df.registerTempTable(\"tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JejvHyxh4_gN",
    "outputId": "71c990e6-f0ab-4949-94f7-5851f094c9d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|num_users|\n",
      "+---------+\n",
      "|    14739|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sql , it shows how many unique user are there in rating dataset\n",
    "num_users = spark.sql(\"SELECT COUNT(DISTINCT userID) AS num_users FROM ratings\")\n",
    "\n",
    "num_users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GdO7Un-X5CKy",
    "outputId": "3aa1da7c-ea14-4e31-dce4-cf9ff3cd31a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14739"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df.select(\"userId\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "l0_zBUH15G0d",
    "outputId": "01542054-c435-49c3-ae71-03e2cb336578"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
       "      pre.function-repr-contents {\n",
       "        overflow-x: auto;\n",
       "        padding: 8px 12px;\n",
       "        max-height: 500px;\n",
       "      }\n",
       "\n",
       "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
       "        cursor: pointer;\n",
       "        max-height: 100px;\n",
       "      }\n",
       "    </style>\n",
       "    <pre style=\"white-space: initial; background:\n",
       "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
       "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.sql.dataframe.DataFrame</b><br/>def __init__(jdf: JavaObject, sql_ctx: Union[&#x27;SQLContext&#x27;, &#x27;SparkSession&#x27;])</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/pyspark/sql/dataframe.py</a>A distributed collection of data grouped into named columns.\n",
       "\n",
       ".. versionadded:: 1.3.0\n",
       "\n",
       ".. versionchanged:: 3.4.0\n",
       "    Supports Spark Connect.\n",
       "\n",
       "Examples\n",
       "--------\n",
       "A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n",
       "and can be created using various functions in :class:`SparkSession`:\n",
       "\n",
       "&gt;&gt;&gt; people = spark.createDataFrame([\n",
       "...     {&quot;deptId&quot;: 1, &quot;age&quot;: 40, &quot;name&quot;: &quot;Hyukjin Kwon&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 50},\n",
       "...     {&quot;deptId&quot;: 1, &quot;age&quot;: 50, &quot;name&quot;: &quot;Takuya Ueshin&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 100},\n",
       "...     {&quot;deptId&quot;: 2, &quot;age&quot;: 60, &quot;name&quot;: &quot;Xinrong Meng&quot;, &quot;gender&quot;: &quot;F&quot;, &quot;salary&quot;: 150},\n",
       "...     {&quot;deptId&quot;: 3, &quot;age&quot;: 20, &quot;name&quot;: &quot;Haejoon Lee&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 200}\n",
       "... ])\n",
       "\n",
       "Once created, it can be manipulated using the various domain-specific-language\n",
       "(DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n",
       "\n",
       "To select a column from the :class:`DataFrame`, use the apply method:\n",
       "\n",
       "&gt;&gt;&gt; age_col = people.age\n",
       "\n",
       "A more concrete example:\n",
       "\n",
       "&gt;&gt;&gt; # To create DataFrame using SparkSession\n",
       "... department = spark.createDataFrame([\n",
       "...     {&quot;id&quot;: 1, &quot;name&quot;: &quot;PySpark&quot;},\n",
       "...     {&quot;id&quot;: 2, &quot;name&quot;: &quot;ML&quot;},\n",
       "...     {&quot;id&quot;: 3, &quot;name&quot;: &quot;Spark SQL&quot;}\n",
       "... ])\n",
       "\n",
       "&gt;&gt;&gt; people.filter(people.age &gt; 30).join(\n",
       "...     department, people.deptId == department.id).groupBy(\n",
       "...     department.name, &quot;gender&quot;).agg({&quot;salary&quot;: &quot;avg&quot;, &quot;age&quot;: &quot;max&quot;}).show()\n",
       "+-------+------+-----------+--------+\n",
       "|   name|gender|avg(salary)|max(age)|\n",
       "+-------+------+-----------+--------+\n",
       "|     ML|     F|      150.0|      60|\n",
       "|PySpark|     M|       75.0|      50|\n",
       "+-------+------+-----------+--------+\n",
       "\n",
       "Notes\n",
       "-----\n",
       "A DataFrame should only be created as described above. It should not be directly\n",
       "created via using the constructor.</pre>\n",
       "      <script>\n",
       "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
       "        for (const element of document.querySelectorAll('.filepath')) {\n",
       "          element.style.display = 'block'\n",
       "          element.onclick = (event) => {\n",
       "            event.preventDefault();\n",
       "            event.stopPropagation();\n",
       "            google.colab.files.view(element.textContent, 80);\n",
       "          };\n",
       "        }\n",
       "      }\n",
       "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
       "        element.onclick = (event) => {\n",
       "          event.preventDefault();\n",
       "          event.stopPropagation();\n",
       "          element.classList.toggle('function-repr-contents-collapsed');\n",
       "        };\n",
       "      }\n",
       "      </script>\n",
       "      </div>"
      ],
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ratings_df.select(\"userId\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJ3NQwpi5IvF"
   },
   "source": [
    "**NO.OF Movies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MuclmYqh5NDn",
    "outputId": "8a173114-6c46-4672-c717-eb78359ce6e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|num_movies|\n",
      "+----------+\n",
      "|     87585|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_movies = spark.sql(\"SELECT count (distinct movieID) as num_movies FROM movies\")\n",
    "num_movies.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XfZCheM95QrO",
    "outputId": "636f7e68-89c0-466e-9551-b46743d57a7b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87585"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df.select('movieID').distinct().count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CFJ8BkZ35SqB",
    "outputId": "691de04b-a534-44a5-e0db-28f9243b7873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many movies are rated by users? 38784\n"
     ]
    }
   ],
   "source": [
    "rated_by_users = ratings_df.select('movieID').distinct().count()\n",
    "print('How many movies are rated by users?', rated_by_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jqs3wGMX5SlK",
    "outputId": "b901aa13-0655-4218-8320-ae5189f96ffa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|movieId|               title|              genres|\n",
      "+-------+--------------------+--------------------+\n",
      "| 100008|    Flaw, The (2011)|         Documentary|\n",
      "| 100015|Chicago Massacre:...|Crime|Drama|Thriller|\n",
      "| 100040|   True Blue (2001) |Crime|Drama|Thriller|\n",
      "| 100042|Guns of Fort Pett...|         War|Western|\n",
      "| 100054| Stella Maris (1918)|               Drama|\n",
      "| 100099|Pictures of the O...|         Documentary|\n",
      "| 100103|Day and Night (Le...|               Drama|\n",
      "| 100131| We the Party (2012)|              Comedy|\n",
      "| 100143|End of Love, The ...|               Drama|\n",
      "| 100161|Oppai Volleyball ...|              Comedy|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unrated_movies_df = movies_df.join(ratings_df, movies_df.movieId == ratings_df.movieId, \"left_anti\")\n",
    "\n",
    "unrated_movies_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "q6ZILbWa8YqP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hjDsn5Gm8Yfk",
    "outputId": "46432a32-a7aa-4e55-b26b-2988003d8643"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              genres|\n",
      "+--------------------+\n",
      "|Comedy|Horror|Thr...|\n",
      "|Adventure|Sci-Fi|...|\n",
      "|Action|Adventure|...|\n",
      "| Action|Drama|Horror|\n",
      "|Comedy|Drama|Horr...|\n",
      "|Adventure|Mystery...|\n",
      "|Children|Comedy|D...|\n",
      "|Action|Animation|...|\n",
      "|Action|Adventure|...|\n",
      "|Animation|Childre...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT DISTINCT genres FROM movies LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uYS5zeht8dud",
    "outputId": "7099c6f6-b810-4799-ff2d-80a0386a02a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|genre      |\n",
      "+-----------+\n",
      "|Action     |\n",
      "|Adventure  |\n",
      "|Animation  |\n",
      "|Children   |\n",
      "|Comedy     |\n",
      "|Crime      |\n",
      "|Documentary|\n",
      "|Drama      |\n",
      "|Fantasy    |\n",
      "|Film-Noir  |\n",
      "|Horror     |\n",
      "|IMAX       |\n",
      "|Musical    |\n",
      "|Mystery    |\n",
      "|Romance    |\n",
      "|Sci-Fi     |\n",
      "|Thriller   |\n",
      "|War        |\n",
      "|Western    |\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, explode, col\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Movie Genres Split\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Example: Load movies dataset\n",
    "# Replace with your actual CSV path\n",
    "movies = spark.read.csv(\"movies.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Split the 'genres' column by '|' and explode into multiple rows\n",
    "genres_exploded = movies.select(explode(split(col(\"genres\"), \"\\\\|\")).alias(\"genre\"))\n",
    "\n",
    "# Remove duplicates and unwanted entries\n",
    "unique_genres_clean = genres_exploded.filter(\n",
    "    (~col(\"genre\").rlike('[()\"]')) &    # Remove parentheses or quotes\n",
    "    (~col(\"genre\").contains('!')) &     # Remove exclamation marks\n",
    "    (~col(\"genre\").contains(',')) &     # Remove commas\n",
    "    (col(\"genre\") != \"\") &              # Remove empty strings\n",
    "    (~col(\"genre\").like('%no genres listed%'))  # Remove placeholder genres\n",
    ").distinct()\n",
    "\n",
    "# Show cleaned unique genres\n",
    "unique_genres_clean.orderBy(\"genre\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v1tgmfcD88vf",
    "outputId": "63a31a7e-9e29-4be8-fe45-058d42e772ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------------------+-------------------------------------------------+\n",
      "|movieId|title                             |genres                                           |\n",
      "+-------+----------------------------------+-------------------------------------------------+\n",
      "|1      |Toy Story (1995)                  |[Adventure, Animation, Children, Comedy, Fantasy]|\n",
      "|2      |Jumanji (1995)                    |[Adventure, Children, Fantasy]                   |\n",
      "|3      |Grumpier Old Men (1995)           |[Comedy, Romance]                                |\n",
      "|4      |Waiting to Exhale (1995)          |[Comedy, Drama, Romance]                         |\n",
      "|5      |Father of the Bride Part II (1995)|[Comedy]                                         |\n",
      "+-------+----------------------------------+-------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+----------------------------------+-------------------------------------------------+\n",
      "|movieId|title                             |genres                                           |\n",
      "+-------+----------------------------------+-------------------------------------------------+\n",
      "|1      |Toy Story (1995)                  |[Adventure, Animation, Children, Comedy, Fantasy]|\n",
      "|2      |Jumanji (1995)                    |[Adventure, Children, Fantasy]                   |\n",
      "|3      |Grumpier Old Men (1995)           |[Comedy, Romance]                                |\n",
      "|4      |Waiting to Exhale (1995)          |[Comedy, Drama, Romance]                         |\n",
      "|5      |Father of the Bride Part II (1995)|[Comedy]                                         |\n",
      "+-------+----------------------------------+-------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, mean, udf, lit, current_timestamp, unix_timestamp, array_contains\n",
    "\n",
    "# UDF to split genres into an array\n",
    "extract_genres = udf(lambda x: x.split(\"|\") if x else [], ArrayType(StringType()))\n",
    "\n",
    "# Create cleaned DataFrame\n",
    "movies_df_clean = movies_df.select(\n",
    "    \"movieId\",\n",
    "    \"title\",\n",
    "    extract_genres(col(\"genres\")).alias(\"genres\")\n",
    ")\n",
    "\n",
    "# Show a few rows\n",
    "movies_df_clean.show(5, truncate=False)\n",
    "\n",
    "# Register as a temp view (optional, for Spark SQL)\n",
    "movies_df_clean.createOrReplaceTempView(\"movies_df_clean\")\n",
    "\n",
    "# Query via SQL\n",
    "spark.sql(\"SELECT * FROM movies_df_clean LIMIT 5\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2CTKVd8g9Vk2",
    "outputId": "ebf30129-91db-4e37-a6b1-041df859d881"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Action',\n",
       " 'Thriller',\n",
       " 'Animation',\n",
       " 'Documentary',\n",
       " 'Sci-Fi',\n",
       " 'Fantasy',\n",
       " 'Comedy',\n",
       " ' We\\'re Comin\\' To Get Ya!\"\" (2014)\"',\n",
       " 'Mystery',\n",
       " 'Drama',\n",
       " 'IMAX',\n",
       " 'War',\n",
       " 'Horror',\n",
       " 'Adventure',\n",
       " 'Film-Noir',\n",
       " 'Romance',\n",
       " 'Musical',\n",
       " 'Children',\n",
       " 'Crime',\n",
       " 'Western',\n",
       " '(no genres listed)']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genres_result = list(set(movies_df_clean.select('genres').rdd.flatMap(tuple).flatMap(tuple).collect()))\n",
    "genres_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "elJrT5a49XtZ",
    "outputId": "e158810c-6e3e-4ba3-c813-cf60fce50449"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Action',\n",
       " 'Thriller',\n",
       " 'Animation',\n",
       " 'Documentary',\n",
       " 'Sci-Fi',\n",
       " 'Fantasy',\n",
       " 'Comedy',\n",
       " ' We\\'re Comin\\' To Get Ya!\"\" (2014)\"',\n",
       " 'Mystery',\n",
       " 'Drama',\n",
       " 'IMAX',\n",
       " 'War',\n",
       " 'Horror',\n",
       " 'Adventure',\n",
       " 'Film-Noir',\n",
       " 'Romance',\n",
       " 'Musical',\n",
       " 'Children',\n",
       " 'Crime',\n",
       " 'Western',\n",
       " '(no genres listed)']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genres_result = list(set(movies_df_clean.select('genres').rdd.flatMap(tuple).flatMap(tuple).collect()))\n",
    "genres_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "id": "ZLipQxam9Z6A",
    "outputId": "2e115cff-1d68-4c2f-b6bf-c1860693ac23"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-d90de53c-93fb-406e-a40b-5d79ee976975\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>We're Comin' To Get Ya!\"\" (2014)\"</th>\n",
       "      <th>(no genres listed)</th>\n",
       "      <th>Action</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Children</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>Crime</th>\n",
       "      <th>Documentary</th>\n",
       "      <th>Drama</th>\n",
       "      <th>...</th>\n",
       "      <th>Film-Noir</th>\n",
       "      <th>Horror</th>\n",
       "      <th>IMAX</th>\n",
       "      <th>Musical</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Sci-Fi</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>War</th>\n",
       "      <th>Western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d90de53c-93fb-406e-a40b-5d79ee976975')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-d90de53c-93fb-406e-a40b-5d79ee976975 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-d90de53c-93fb-406e-a40b-5d79ee976975');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-ab5faa44-2114-4be9-983d-4d8fd032e754\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ab5faa44-2114-4be9-983d-4d8fd032e754')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-ab5faa44-2114-4be9-983d-4d8fd032e754 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "    We're Comin' To Get Ya!\"\" (2014)\"  (no genres listed)  Action  Adventure  \\\n",
       "0                                   0                   0       0          1   \n",
       "1                                   0                   0       0          1   \n",
       "2                                   0                   0       0          0   \n",
       "3                                   0                   0       0          0   \n",
       "4                                   0                   0       0          0   \n",
       "\n",
       "   Animation  Children  Comedy  Crime  Documentary  Drama  ...  Film-Noir  \\\n",
       "0          1         1       1      0            0      0  ...          0   \n",
       "1          0         1       0      0            0      0  ...          0   \n",
       "2          0         0       1      0            0      0  ...          0   \n",
       "3          0         0       1      0            0      1  ...          0   \n",
       "4          0         0       1      0            0      0  ...          0   \n",
       "\n",
       "   Horror  IMAX  Musical  Mystery  Romance  Sci-Fi  Thriller  War  Western  \n",
       "0       0     0        0        0        0       0         0    0        0  \n",
       "1       0     0        0        0        0       0         0    0        0  \n",
       "2       0     0        0        0        1       0         0    0        0  \n",
       "3       0     0        0        0        1       0         0    0        0  \n",
       "4       0     0        0        0        0       0         0    0        0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_pdf = movies_df.toPandas()\n",
    "movie_pdf['genres'].str.get_dummies(sep='|').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "0sK0Ge1L9cFo"
   },
   "outputs": [],
   "source": [
    "list_of_movie = list(movie_pdf['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IDKpjpvJ9eUq",
    "outputId": "3f65963c-5ddb-4321-f3bd-a3fe5182ef51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|1     |17     |4.0   |944249077|\n",
      "|1     |25     |1.0   |944250228|\n",
      "|1     |29     |2.0   |943230976|\n",
      "|1     |30     |5.0   |944249077|\n",
      "|1     |32     |5.0   |943228858|\n",
      "+------+-------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ratings_df.show(10)\n",
    "\n",
    "# Register DataFrame as a SQL view\n",
    "ratings_df.createOrReplaceTempView(\"ratings_df\")\n",
    "\n",
    "# Use Spark SQL to preview\n",
    "spark.sql(\"SELECT * FROM ratings_df LIMIT 5\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "FCKCeyYJ9x4U"
   },
   "outputs": [],
   "source": [
    "movie_ratings=ratings_df.drop('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "MEaINxXu9zTo"
   },
   "outputs": [],
   "source": [
    "# Data type convert\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "movie_ratings = movie_ratings.withColumn(\"userId\", movie_ratings[\"userId\"].cast(IntegerType()))\n",
    "movie_ratings = movie_ratings.withColumn(\"movieId\", movie_ratings[\"movieId\"].cast(IntegerType()))\n",
    "movie_ratings = movie_ratings.withColumn(\"rating\", movie_ratings[\"rating\"].cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pwleY2Qv94Nz",
    "outputId": "c46b8a3b-1c1f-4cd8-f46f-b181d45839a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|1     |17     |4.0   |\n",
      "|1     |25     |1.0   |\n",
      "|1     |29     |2.0   |\n",
      "|1     |30     |5.0   |\n",
      "|1     |32     |5.0   |\n",
      "|1     |34     |2.0   |\n",
      "|1     |36     |1.0   |\n",
      "|1     |80     |5.0   |\n",
      "|1     |110    |3.0   |\n",
      "|1     |111    |5.0   |\n",
      "+------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register DataFrame as SQL view\n",
    "movie_ratings.createOrReplaceTempView(\"movie_ratings\")\n",
    "\n",
    "# Use Spark SQL to preview data\n",
    "spark.sql(\"SELECT * FROM movie_ratings LIMIT 10\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "p5mjn7IV97Mb"
   },
   "outputs": [],
   "source": [
    "# import package\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import CrossValidator,ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "UjhxWDvv99Gf"
   },
   "outputs": [],
   "source": [
    "#Create test and train set\n",
    "(training,test)=movie_ratings.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "NTVtJ4XZ9_Cx"
   },
   "outputs": [],
   "source": [
    "# Create ALS model\n",
    "# Build the recommendation model using ALS on the training data\n",
    "# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "als = ALS(maxIter=5, rank=10, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
    "          coldStartStrategy=\"drop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "115wTRa1-Bcp",
    "outputId": "252ce072-a939-4584-8729-d4da3ab05393"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: alpha for implicit preference (default: 1.0)\n",
      "blockSize: block size for stacking input data in matrices. Data is stacked within partitions. If block size is more than remaining data in a partition then it is adjusted to the size of this data. (default: 4096)\n",
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\n",
      "coldStartStrategy: strategy for dealing with unknown or new users/items at prediction time. This may be useful in cross-validation or production scenarios, for handling user/item ids the model has not seen in the training data. Supported values: 'nan', 'drop'. (default: nan, current: drop)\n",
      "finalStorageLevel: StorageLevel for ALS model factors. (default: MEMORY_AND_DISK)\n",
      "implicitPrefs: whether to use implicit preference (default: False)\n",
      "intermediateStorageLevel: StorageLevel for intermediate datasets. Cannot be 'NONE'. (default: MEMORY_AND_DISK)\n",
      "itemCol: column name for item ids. Ids must be within the integer value range. (default: item, current: movieId)\n",
      "maxIter: max number of iterations (>= 0). (default: 10, current: 5)\n",
      "nonnegative: whether to use nonnegative constraint for least squares (default: False)\n",
      "numItemBlocks: number of item blocks (default: 10)\n",
      "numUserBlocks: number of user blocks (default: 10)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "rank: rank of the factorization (default: 10, current: 10)\n",
      "ratingCol: column name for ratings (default: rating, current: rating)\n",
      "regParam: regularization parameter (>= 0). (default: 0.1, current: 0.01)\n",
      "seed: random seed. (default: 4552126003571142724)\n",
      "userCol: column name for user ids. Ids must be within the integer value range. (default: user, current: userId)\n"
     ]
    }
   ],
   "source": [
    "# 1st print a list of parameters\n",
    "print(als.explainParams())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "Dw2hXTaQ-FfQ"
   },
   "outputs": [],
   "source": [
    "#Tune model using ParamGridBuilder\n",
    "# it will take long time in the cv period, so just use few parameter to try\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(als.regParam, [0.01])\n",
    "             .addGrid(als.rank, [10])\n",
    "             .addGrid(als.maxIter, [15])\n",
    "             .build())\n",
    "\n",
    "# paramGrid = (ParamGridBuilder()\n",
    "#              .addGrid(als.regParam, [0.01, 0.5, 1, 1.5])\n",
    "#              .addGrid(als.rank, [10, 15, 20, 25])\n",
    "#              .addGrid(als.maxIter, [1, 5, 10, 15])\n",
    "#              .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "zUjejV1o-Hh-"
   },
   "outputs": [],
   "source": [
    "# Define evaluator as RMSE\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "RBtGbCXI-KVr"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "# Build Cross validation\n",
    "# Create 5-fold CrossValidator\n",
    "# it takes too long that I only use 2-fold\n",
    "cv = CrossValidator(estimator=als, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=2)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(training)\n",
    "# this will likely take a fair amount of time because of the amount of models that we're creating and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "8Ms42Yl8-KRX"
   },
   "outputs": [],
   "source": [
    "# Extract the best model selected by CV\n",
    "best_model = cvModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vS2QNEZr-Ous",
    "outputId": "1c3c9058-774a-4592-8d54-d7cfe09292f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Best Model**\n",
      "Rank:  ALSModel: uid=ALS_4ad983ccc103, rank=10\n",
      " MaxIter:  15\n",
      " RegParam: ALS_4ad983ccc103__regParam\n"
     ]
    }
   ],
   "source": [
    "#Fit ALS model to training data\n",
    "\n",
    "# specify parameter settings by the best model obtained via CV\n",
    "print (\"**Best Model**\")\n",
    "print (\"Rank: \", best_model)\n",
    "print (\" MaxIter: \", str(best_model._java_obj.parent().getMaxIter()))\n",
    "print (\" RegParam:\",  best_model._java_obj.parent().regParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "MwBzjE3v-Orp"
   },
   "outputs": [],
   "source": [
    "#Generate predictions and evaluate using RMSE\n",
    "predictions=best_model.transform(test)\n",
    "rmse = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yYl4sW2y-Opu",
    "outputId": "f7d8a96f-4f4b-48fe-f682-1d14afe252aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 0.8489625613296988\n"
     ]
    }
   ],
   "source": [
    "#Print RMSE\n",
    "print (\"RMSE = \"+str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "h0pgnopF-Ubf"
   },
   "outputs": [],
   "source": [
    "#Extract best model from the tuning exercise using ParamGridBuilder\n",
    "\n",
    "als_best = ALS(maxIter=15, rank=10, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "model = als_best.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FMdZfu54-WFM",
    "outputId": "8a7ad021-d56b-4df4-cfbb-8dd15fdfa1f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating|prediction|\n",
      "+------+-------+------+----------+\n",
      "|148   |2366   |1.0   |1.2940143 |\n",
      "|496   |1580   |3.5   |3.6057587 |\n",
      "|833   |44022  |4.0   |3.2705681 |\n",
      "|4935  |1645   |5.0   |5.0059037 |\n",
      "|4935  |3175   |4.5   |4.662366  |\n",
      "|6466  |1088   |2.0   |3.284985  |\n",
      "|7982  |1645   |1.0   |4.4000382 |\n",
      "|8086  |1580   |5.0   |4.1907616 |\n",
      "|8086  |1645   |4.0   |4.209381  |\n",
      "|540   |1591   |4.0   |3.6335447 |\n",
      "+------+-------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register DataFrame as a temporary SQL view\n",
    "predictions.createOrReplaceTempView(\"predictions\")\n",
    "\n",
    "# Query and show first 10 rows\n",
    "spark.sql(\"SELECT * FROM predictions LIMIT 10\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fl0Xrdn8-Ym9",
    "outputId": "9b7802d6-50e3-4e59-8025-99bd9d15169e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 0.7077683395258209\n"
     ]
    }
   ],
   "source": [
    "alldata=best_model.transform(movie_ratings)\n",
    "rmse = evaluator.evaluate(alldata)\n",
    "print (\"RMSE = \"+str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u7ONYHhy-Yis",
    "outputId": "18dbcfbf-fc58-46f4-fc2e-df3a6f538508"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pyspark/sql/dataframe.py:329: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "alldata.registerTempTable(\"alldata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DsemG9PGBjHq",
    "outputId": "cacf59f6-3f31-4e99-c978-a21c8ff56b4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating|prediction|\n",
      "+------+-------+------+----------+\n",
      "|148   |1      |1.0   |1.8346332 |\n",
      "|148   |2      |2.5   |1.7208279 |\n",
      "|148   |10     |4.0   |3.1920507 |\n",
      "|148   |47     |3.5   |3.0077496 |\n",
      "|148   |50     |4.0   |3.2468784 |\n",
      "|148   |110    |4.0   |3.5731668 |\n",
      "|148   |111    |3.0   |1.8662812 |\n",
      "|148   |165    |4.0   |3.3998346 |\n",
      "|148   |260    |4.5   |3.9098592 |\n",
      "|148   |288    |1.0   |2.142435  |\n",
      "+------+-------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM alldata LIMIT 10\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F_KNyU-VBp72",
    "outputId": "7f61e13c-4dc5-4720-9d5f-36c87b0d6611"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------------------------+-------------------------------------------+------+-------+------+----------+\n",
      "|movieId|title                                    |genres                                     |userId|movieId|rating|prediction|\n",
      "+-------+-----------------------------------------+-------------------------------------------+------+-------+------+----------+\n",
      "|1      |Toy Story (1995)                         |Adventure|Animation|Children|Comedy|Fantasy|148   |1      |1.0   |1.8346332 |\n",
      "|2      |Jumanji (1995)                           |Adventure|Children|Fantasy                 |148   |2      |2.5   |1.7208279 |\n",
      "|10     |GoldenEye (1995)                         |Action|Adventure|Thriller                  |148   |10     |4.0   |3.1920507 |\n",
      "|47     |Seven (a.k.a. Se7en) (1995)              |Mystery|Thriller                           |148   |47     |3.5   |3.0077496 |\n",
      "|50     |Usual Suspects, The (1995)               |Crime|Mystery|Thriller                     |148   |50     |4.0   |3.2468784 |\n",
      "|110    |Braveheart (1995)                        |Action|Drama|War                           |148   |110    |4.0   |3.5731668 |\n",
      "|111    |Taxi Driver (1976)                       |Crime|Drama|Thriller                       |148   |111    |3.0   |1.8662812 |\n",
      "|165    |Die Hard: With a Vengeance (1995)        |Action|Crime|Thriller                      |148   |165    |4.0   |3.3998346 |\n",
      "|260    |Star Wars: Episode IV - A New Hope (1977)|Action|Adventure|Sci-Fi                    |148   |260    |4.5   |3.9098592 |\n",
      "|288    |Natural Born Killers (1994)              |Action|Crime|Thriller                      |148   |288    |1.0   |2.142435  |\n",
      "+-------+-----------------------------------------+-------------------------------------------+------+-------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM movies\n",
    "    JOIN alldata\n",
    "    ON movies.movieId = alldata.movieId\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ds0C-JVBtRb",
    "outputId": "aee2d7ea-cbf3-4cba-c560-c747b8fd73ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|userId|recommendations                                                                                                                                                                                         |\n",
      "+------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1     |[{201444, 10.871121}, {194680, 8.730726}, {6840, 8.410723}, {167184, 8.120133}, {988, 7.616467}, {267654, 7.602745}, {148480, 7.425955}, {171695, 7.3789773}, {56931, 7.378093}, {280100, 7.309625}]    |\n",
      "|3     |[{280310, 11.663724}, {5622, 9.093227}, {2744, 9.055946}, {4877, 8.484942}, {187815, 8.45415}, {191811, 8.105355}, {33124, 8.084288}, {131826, 8.074549}, {6892, 8.014042}, {7440, 7.9827037}]          |\n",
      "|5     |[{148426, 13.199804}, {165959, 12.3068285}, {136515, 11.808568}, {4588, 10.666239}, {98981, 9.839717}, {148480, 9.77015}, {194680, 9.635307}, {141668, 9.546212}, {79006, 8.990754}, {144490, 8.615462}]|\n",
      "|6     |[{5622, 16.96626}, {38656, 15.55437}, {26269, 14.792275}, {54934, 14.246196}, {50651, 14.21075}, {7179, 13.9215975}, {287591, 13.8660965}, {167636, 13.670487}, {563, 13.321751}, {143511, 13.262261}]  |\n",
      "|9     |[{7440, 7.7169385}, {58898, 7.58553}, {194680, 7.4881177}, {6065, 7.479752}, {287591, 7.370899}, {91423, 7.2299423}, {93320, 7.22994}, {563, 7.199446}, {167470, 7.1685247}, {6159, 7.090989}]          |\n",
      "|12    |[{831, 10.048003}, {201242, 10.047516}, {5153, 9.939301}, {61167, 9.922574}, {4740, 9.712902}, {42385, 9.433491}, {7636, 8.93408}, {100583, 8.933716}, {54934, 8.692159}, {39398, 8.692092}]            |\n",
      "|13    |[{7351, 9.848077}, {108579, 8.909963}, {148426, 8.88005}, {90929, 8.791142}, {165959, 8.66259}, {183855, 8.509836}, {51207, 8.316789}, {86574, 8.223593}, {5366, 8.0120535}, {200864, 7.8432255}]       |\n",
      "|15    |[{280310, 10.846419}, {7440, 10.299529}, {50651, 9.68202}, {5622, 9.437517}, {6159, 9.007415}, {152284, 8.878926}, {5839, 8.714186}, {4877, 8.651015}, {170425, 8.516672}, {1696, 8.466551}]            |\n",
      "|16    |[{2744, 9.633983}, {140805, 9.024978}, {4201, 8.533913}, {6159, 8.419668}, {132883, 8.267935}, {5622, 7.7882485}, {108056, 7.733783}, {193445, 7.733233}, {62577, 7.5643597}, {58111, 7.410723}]        |\n",
      "|17    |[{172789, 8.497601}, {108056, 8.300212}, {193445, 8.244698}, {6065, 8.155999}, {6844, 8.155877}, {280310, 7.685298}, {287591, 7.633438}, {91423, 7.5845127}, {6159, 7.5743923}, {89985, 7.4531302}]     |\n",
      "+------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate top 10 movie recommendations for each user\n",
    "user_recs = best_model.recommendForAllUsers(10)\n",
    "\n",
    "# Register as SQL view (optional)\n",
    "user_recs.createOrReplaceTempView(\"user_recs\")\n",
    "\n",
    "# Use Spark SQL to display first 10 users' recommendations\n",
    "spark.sql(\"SELECT * FROM user_recs LIMIT 10\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6eYwK0jB30_",
    "outputId": "ad0b251e-d05f-4bdc-e0fc-010b81d0e4ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(userId=1, recommendations=[Row(movieId=201444, rating=10.871121406555176), Row(movieId=194680, rating=8.73072624206543), Row(movieId=6840, rating=8.410722732543945), Row(movieId=167184, rating=8.120133399963379), Row(movieId=988, rating=7.616466999053955), Row(movieId=267654, rating=7.602745056152344), Row(movieId=148480, rating=7.425954818725586), Row(movieId=171695, rating=7.378977298736572), Row(movieId=56931, rating=7.3780927658081055), Row(movieId=280100, rating=7.309625148773193)])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_recs.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "HswYR14JB5pi"
   },
   "outputs": [],
   "source": [
    "user_recs.registerTempTable(\"als_recs_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O4U3elN0B9en",
    "outputId": "396ab6ab-e854-4dcf-e75d-400180d8895b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|userId|recommendations                                                                                                                                                                                                                                    |\n",
      "+------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1     |[List(201444, 10.871121), List(194680, 8.730726), List(6840, 8.410723), List(167184, 8.120133), List(988, 7.616467), List(267654, 7.602745), List(148480, 7.425955), List(171695, 7.3789773), List(56931, 7.378093), List(280100, 7.309625)]       |\n",
      "|3     |[List(280310, 11.663724), List(5622, 9.093227), List(2744, 9.055946), List(4877, 8.484942), List(187815, 8.45415), List(191811, 8.105355), List(33124, 8.084288), List(131826, 8.074549), List(6892, 8.014042), List(7440, 7.9827037)]             |\n",
      "|5     |[List(148426, 13.199804), List(165959, 12.3068285), List(136515, 11.808568), List(4588, 10.666239), List(98981, 9.839717), List(148480, 9.77015), List(194680, 9.635307), List(141668, 9.546212), List(79006, 8.990754), List(144490, 8.615462)]   |\n",
      "|6     |[List(5622, 16.96626), List(38656, 15.55437), List(26269, 14.792275), List(54934, 14.246196), List(50651, 14.21075), List(7179, 13.9215975), List(287591, 13.8660965), List(167636, 13.670487), List(563, 13.321751), List(143511, 13.262261)]     |\n",
      "|9     |[List(7440, 7.7169385), List(58898, 7.58553), List(194680, 7.4881177), List(6065, 7.479752), List(287591, 7.370899), List(91423, 7.2299423), List(93320, 7.22994), List(563, 7.199446), List(167470, 7.1685247), List(6159, 7.090989)]             |\n",
      "|12    |[List(831, 10.048003), List(201242, 10.047516), List(5153, 9.939301), List(61167, 9.922574), List(4740, 9.712902), List(42385, 9.433491), List(7636, 8.93408), List(100583, 8.933716), List(54934, 8.692159), List(39398, 8.692092)]               |\n",
      "|13    |[List(7351, 9.848077), List(108579, 8.909963), List(148426, 8.88005), List(90929, 8.791142), List(165959, 8.66259), List(183855, 8.509836), List(51207, 8.316789), List(86574, 8.223593), List(5366, 8.0120535), List(200864, 7.8432255)]          |\n",
      "|15    |[List(280310, 10.846419), List(7440, 10.299529), List(50651, 9.68202), List(5622, 9.437517), List(6159, 9.007415), List(152284, 8.878926), List(5839, 8.714186), List(4877, 8.651015), List(170425, 8.516672), List(1696, 8.466551)]               |\n",
      "|16    |[List(2744, 9.633983), List(140805, 9.024978), List(4201, 8.533913), List(6159, 8.419668), List(132883, 8.267935), List(5622, 7.7882485), List(108056, 7.733783), List(193445, 7.733233), List(62577, 7.5643597), List(58111, 7.410723)]           |\n",
      "|17    |[List(172789, 8.497601), List(108056, 8.300212), List(193445, 8.244698), List(6065, 8.155999), List(6844, 8.155877), List(280310, 7.685298), List(287591, 7.633438), List(91423, 7.5845127), List(6159, 7.5743923), List(89985, 7.4531302)]        |\n",
      "|19    |[List(194680, 10.189362), List(181671, 9.96053), List(148480, 9.553716), List(201444, 9.045657), List(4687, 8.825346), List(76776, 8.583339), List(167470, 8.474251), List(7334, 8.377871), List(167184, 8.271834), List(4472, 8.201691)]          |\n",
      "|20    |[List(251396, 7.263438), List(194680, 6.684445), List(68874, 6.4333935), List(8486, 6.36608), List(72037, 6.179102), List(136844, 6.1171327), List(201444, 6.049346), List(169912, 5.9860754), List(90929, 5.929271), List(54229, 5.8844104)]      |\n",
      "|22    |[List(148426, 8.798018), List(194680, 8.151986), List(3245, 7.3703856), List(87004, 7.1443257), List(98973, 7.126345), List(5224, 7.118968), List(6144, 6.873687), List(223468, 6.8654118), List(91423, 6.8409677), List(666, 6.788805)]           |\n",
      "|26    |[List(196937, 9.2449045), List(26379, 8.953378), List(2930, 8.615925), List(42385, 8.089689), List(168420, 8.010036), List(32582, 7.993201), List(92154, 7.908414), List(171007, 7.8793283), List(336, 7.6849337), List(4715, 7.6756234)]          |\n",
      "|27    |[List(91423, 6.827325), List(194680, 6.5024524), List(6144, 6.288437), List(6065, 6.0721774), List(78438, 5.9825664), List(33945, 5.7019887), List(280310, 5.6855702), List(6823, 5.685274), List(107505, 5.599195), List(185365, 5.583483)]       |\n",
      "|28    |[List(251396, 6.3399997), List(225758, 6.1037316), List(82976, 6.089174), List(162344, 6.019397), List(172789, 5.987023), List(280310, 5.9709907), List(78438, 5.934659), List(180967, 5.8041897), List(250764, 5.802778), List(123215, 5.7917314)]|\n",
      "|31    |[List(6844, 8.856131), List(171695, 8.670877), List(2930, 8.632764), List(128089, 7.872432), List(1038, 7.8443503), List(95488, 7.819512), List(106983, 7.7463365), List(7141, 7.7451496), List(196937, 7.6722836), List(212024, 7.6519485)]       |\n",
      "|34    |[List(194680, 7.357151), List(91423, 7.3206487), List(2744, 6.9442368), List(138835, 6.94047), List(1780, 6.9388714), List(98981, 6.8692713), List(33945, 6.81682), List(280310, 6.699949), List(49225, 6.4137726), List(201861, 6.2365947)]       |\n",
      "|35    |[List(171695, 7.2656507), List(194680, 6.6428194), List(5953, 6.579259), List(167470, 6.4633245), List(128852, 6.031456), List(137904, 5.997225), List(39398, 5.982634), List(91423, 5.952503), List(105250, 5.9131413), List(5916, 5.801788)]     |\n",
      "|37    |[List(60482, 6.3948507), List(4877, 6.1643615), List(6065, 6.010726), List(172789, 5.9928923), List(91423, 5.951246), List(287591, 5.912141), List(78009, 5.855639), List(161584, 5.7660565), List(26259, 5.748338), List(133185, 5.662082)]       |\n",
      "+------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, concat, lit, col, collect_list\n",
    "\n",
    "# Step 1: Explode the recommendations\n",
    "explode_rec = spark.sql('''\n",
    "    SELECT userId,\n",
    "           explode(recommendations) AS MovieRec\n",
    "    FROM als_recs_temp\n",
    "''')\n",
    "\n",
    "explode_rec.createOrReplaceTempView(\"explode_rec\")\n",
    "\n",
    "# Step 2: Convert each struct into string like \"List(movieId, rating)\"\n",
    "formatted = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        userId,\n",
    "        concat('List(', MovieRec.movieId, ', ', MovieRec.rating, ')') AS MovieRec\n",
    "    FROM explode_rec\n",
    "\"\"\")\n",
    "\n",
    "# Step 3: Group them back into a list for each user\n",
    "from pyspark.sql.functions import collect_list\n",
    "\n",
    "recommendation_list = formatted.groupBy(\"userId\") \\\n",
    "                               .agg(collect_list(\"MovieRec\").alias(\"recommendations\"))\n",
    "\n",
    "# Step 4: Show the final output\n",
    "recommendation_list.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "NiGoKGZXEtar"
   },
   "outputs": [],
   "source": [
    "fianl_recs = spark.sql(\"SELECT userId,\\\n",
    "                               movieIds_and_ratings.movieId AS movieId,\\\n",
    "                               movieIds_and_ratings.rating AS prediction\\\n",
    "                               FROM als_recs_temp\\\n",
    "                               LATERAL VIEW explode(recommendations) exploded_table AS movieIds_and_ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5kDNQbrqEvjW",
    "outputId": "b47d8b31-df01-487c-a07d-322b76271019"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+\n",
      "|userId|movieId|prediction|\n",
      "+------+-------+----------+\n",
      "|1     |201444 |10.871121 |\n",
      "|1     |194680 |8.730726  |\n",
      "|1     |6840   |8.410723  |\n",
      "|1     |167184 |8.120133  |\n",
      "|1     |988    |7.616467  |\n",
      "|1     |267654 |7.602745  |\n",
      "|1     |148480 |7.425955  |\n",
      "|1     |171695 |7.3789773 |\n",
      "|1     |56931  |7.378093  |\n",
      "|1     |280100 |7.309625  |\n",
      "+------+-------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+------+-------+----------+\n",
      "|userId|movieId|prediction|\n",
      "+------+-------+----------+\n",
      "|1     |201444 |10.871121 |\n",
      "|1     |194680 |8.730726  |\n",
      "|1     |6840   |8.410723  |\n",
      "|1     |167184 |8.120133  |\n",
      "|1     |988    |7.616467  |\n",
      "|1     |267654 |7.602745  |\n",
      "|1     |148480 |7.425955  |\n",
      "|1     |171695 |7.3789773 |\n",
      "|1     |56931  |7.378093  |\n",
      "|1     |280100 |7.309625  |\n",
      "+------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming fianl_recs DataFrame already exists\n",
    "# (for example, from your previous steps)\n",
    "\n",
    "# Show the first 10 rows\n",
    "fianl_recs.show(10, truncate=False)\n",
    "\n",
    "# Register as a temporary SQL view\n",
    "fianl_recs.createOrReplaceTempView(\"fianl_recs\")\n",
    "\n",
    "# Run a SQL query and display the results using show()\n",
    "result = spark.sql(\"SELECT * FROM fianl_recs LIMIT 10\")\n",
    "result.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wp2ttDX4Ghun",
    "outputId": "18114de1-bcb1-49d3-a91e-b51a6640efa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+------+\n",
      "|userId|movieId|prediction|rating|\n",
      "+------+-------+----------+------+\n",
      "|1     |988    |7.616467  |NULL  |\n",
      "|1     |56931  |7.378093  |NULL  |\n",
      "|1     |148480 |7.425955  |NULL  |\n",
      "|1     |167184 |8.120133  |NULL  |\n",
      "|1     |267654 |7.602745  |NULL  |\n",
      "+------+-------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join ALS recommendations with movie ratings\n",
    "# Keep only movies the user has NOT rated yet (rating is NULL)\n",
    "final_rec = fianl_recs.join(\n",
    "    movie_ratings,\n",
    "    ['userId', 'movieId'],\n",
    "    'left'\n",
    ").filter(movie_ratings.rating.isNull())\n",
    "\n",
    "# Register as SQL temp view\n",
    "final_rec.createOrReplaceTempView(\"final_rec\")\n",
    "\n",
    "# Query and show top 5 recommendations\n",
    "result = spark.sql(\"SELECT * FROM final_rec LIMIT 5\")\n",
    "result.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "yGmzZqmpGjY3"
   },
   "outputs": [],
   "source": [
    "final_rec.registerTempTable(\"final_rec\")\n",
    "movies_df.registerTempTable(\"movies_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DTi3zHcFHSLd",
    "outputId": "f6fbdf6b-ebf7-4a14-8489-e529d72b5300"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------------------------------------------------+\n",
      "|userId|title                                                            |\n",
      "+------+-----------------------------------------------------------------+\n",
      "|575   |Deterrence (1999)                                                |\n",
      "|575   |King of Kings (1961)                                             |\n",
      "|575   |Unprecedented: The 2000 Presidential Election (2002)             |\n",
      "|575   |What's in a Name (Prénom, Le) (2012)                             |\n",
      "|575   |Angel's Egg (Tenshi no tamago) (1985)                            |\n",
      "|575   |Skinamarink (2022)                                               |\n",
      "|575   |Canal, The (2014)                                                |\n",
      "|575   |Shadows of Our Forgotten Ancestors (Tini zabutykh predkiv) (1964)|\n",
      "|575   |Paper Clips (2004)                                               |\n",
      "|575   |Goodbye, Dragon Inn (Bu san) (2003)                              |\n",
      "+------+-----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make sure both DataFrames are registered as temp views\n",
    "final_rec.createOrReplaceTempView(\"final_rec\")\n",
    "movies_df.createOrReplaceTempView(\"movies_df\")\n",
    "\n",
    "# Run the same SQL query using spark.sql()\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        t1.userId,\n",
    "        t2.title\n",
    "    FROM final_rec t1\n",
    "    LEFT JOIN movies_df t2\n",
    "    ON t1.movieId = t2.movieId\n",
    "    WHERE t1.userId = 575\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "# Show the result\n",
    "result.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U6eq1X1qHcVl",
    "outputId": "358306b0-ecf2-4b29-d451-20937ccb1460"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------------------+\n",
      "|userId|title                               |\n",
      "+------+------------------------------------+\n",
      "|273   |Jeff Dunham: Controlled Chaos (2011)|\n",
      "|273   |Don't Hug Me I'm Scared 6 (2016)    |\n",
      "|273   |New Land, The (Nybyggarna) (1972)   |\n",
      "|273   |99 and 44/100% Dead (1974)          |\n",
      "|273   |Otello (1986)                       |\n",
      "+------+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register DataFrames as SQL views (only once if not already done)\n",
    "final_rec.createOrReplaceTempView(\"final_rec\")\n",
    "movies_df.createOrReplaceTempView(\"movies_df\")\n",
    "\n",
    "# Run your SQL query via PySpark\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        t1.userId,\n",
    "        t2.title\n",
    "    FROM final_rec t1\n",
    "    LEFT JOIN movies_df t2\n",
    "        ON t1.movieId = t2.movieId\n",
    "    WHERE t1.userId = 273\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "# Show the results\n",
    "result.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V_-NjQquHfGy",
    "outputId": "78e63da0-1e3c-4167-9dd7-67a26222b6b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38784"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1st extract productFeatures matrix\n",
    "# The productFeatures matrix will be used to create an item-item collaborative filtering recommendation model\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "import math\n",
    "\n",
    "model_a = ALS.train(movie_ratings, rank=10, iterations=15,\n",
    "                      lambda_=0.01)\n",
    "model_a.productFeatures().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "hqarMYWLHhEx"
   },
   "outputs": [],
   "source": [
    "# look at the feature vector of movie 463\n",
    "movie_feature = model_a.productFeatures().lookup(471)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "OJ4Ry6SHHlo5"
   },
   "outputs": [],
   "source": [
    "# Next define cosine similarity function to measure movie similarity\n",
    "def cosineSimilarity(vec1, vec2):\n",
    "  return vec1.dot(vec2) / (LA.norm(vec1) * LA.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cQ0EIJSNH0Kq",
    "outputId": "a8cd9d55-1838-4e57-fd3d-42e067488448"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'Toy Story (1995)'), (2, 'Jumanji (1995)'), (3, 'Grumpier Old Men (1995)'), (4, 'Waiting to Exhale (1995)'), (5, 'Father of the Bride Part II (1995)')]\n"
     ]
    }
   ],
   "source": [
    "# ✅ Correct version for Google Colab\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Create Spark session if not already created\n",
    "spark = SparkSession.builder.appName(\"MovieRecommendation\").getOrCreate()\n",
    "sc = spark.sparkContext  # get SparkContext\n",
    "\n",
    "# ✅ Make sure movies.csv path is correct\n",
    "movies_file = \"/content/movies.csv\"  # assuming movies.csv is in /content\n",
    "\n",
    "# Read file using SparkContext\n",
    "movies_sc = sc.textFile(movies_file)\n",
    "\n",
    "# Extract header\n",
    "movies_sc_header = movies_sc.first()\n",
    "\n",
    "# Parse and cache movie data (movieId, title)\n",
    "movies_data = (movies_sc\n",
    "    .filter(lambda line: line != movies_sc_header)\n",
    "    .map(lambda line: line.split(\",\"))  # split CSV by commas\n",
    "    .map(lambda tokens: (tokens[0], tokens[1]))  # (movieId, title)\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "# Convert movieId to int\n",
    "movies_titles = movies_data.map(lambda x: (int(x[0]), x[1]))\n",
    "\n",
    "# Show a few entries\n",
    "print(movies_titles.take(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "ZRerFBJeI-N9",
    "outputId": "aca171a7-d579-4ce0-f2d0-0776b2cdc135"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'movieId,title,genres'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_sc_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r5uIoy9eJAJ_",
    "outputId": "3eb266d0-71d3-4c38-ca16-44016e5efdaf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[2231] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YarMQDevJCyQ",
    "outputId": "95e6f391-631a-4d5b-f0dd-9ebf2fbfd12e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[2233] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "hkPtj8XxJE2O"
   },
   "outputs": [],
   "source": [
    "# Build similarity matrix for movieid 471 using the product features matrix\n",
    "\n",
    "similarMovies = model_a.productFeatures().map(lambda products:(products[0],\n",
    "                                        cosineSimilarity(np.asarray(products[1]), movie_feature))).join(movies_titles).map(lambda r: (r[1][1], r[1][0], r[0]))\n",
    "\n",
    "# Sort the top 10 most similar movies descendingly by cosine similarity measure\n",
    "# similarMovies.takeOrdered(11, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "SgKrhxsGJGq0"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nZnj2mPCJIIy",
    "outputId": "47bbb414-7611-4617-f99b-bc0dd10b4648"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------------------------------------------------------------------------------------------------------------+\n",
      "|id |features                                                                                                                   |\n",
      "+---+---------------------------------------------------------------------------------------------------------------------------+\n",
      "|10 |[-0.0648435, -0.9313248, -0.35683972, -0.8012625, 0.49968833, -0.035513002, -0.18069409, 0.06488603, 0.9119343, 0.18105297]|\n",
      "|20 |[-0.5296203, -0.6894077, 0.087655954, -0.5194098, 0.5146937, -0.073612325, -0.045994196, 0.8086063, 0.3980608, 0.41987616] |\n",
      "|30 |[-0.90422314, -0.7759971, -0.59957737, -0.68116117, 0.10081761, 0.9508019, -1.3112195, 0.112045445, 0.53463894, 0.7831686] |\n",
      "|40 |[0.95531356, -1.2345128, -0.3409756, -0.30253667, 0.16364394, 0.17782375, -1.4876945, 0.46898955, 0.9028022, 0.20169705]   |\n",
      "|50 |[0.39132077, -0.47582763, -0.12869772, -1.1095624, 0.48524442, 0.8662814, 0.10703467, 0.27376524, 0.8442102, 0.5690098]    |\n",
      "+---+---------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming you already have 'best_model' from ALS training\n",
    "\n",
    "# Extract item factors (movie latent features)\n",
    "a = best_model.itemFactors\n",
    "\n",
    "# Register as SQL temporary view\n",
    "a.createOrReplaceTempView(\"a\")\n",
    "\n",
    "# ✅ Run SQL query using spark.sql()\n",
    "result_df = spark.sql(\"SELECT * FROM a LIMIT 5\")\n",
    "\n",
    "# ✅ Show result in Colab\n",
    "result_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Xlehh9VJb6F",
    "outputId": "fb0f2ed3-18ab-4706-8498-a8ab6a54cd4b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pyspark/sql/dataframe.py:329: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "a.registerTempTable(\"movie_on_movie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205
    },
    "id": "npLk4GFLJj4Z",
    "outputId": "94248bea-e8a4-495f-b38a-59dddb7a19dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------+\n",
      "|features                                                                                                              |\n",
      "+----------------------------------------------------------------------------------------------------------------------+\n",
      "|[0.73430204, -0.64449453, -0.6654818, -0.74435824, 0.22103347, 0.3214821, -0.4667724, 0.4513862, 0.63497394, 0.817736]|\n",
      "+----------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"result_df\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"features\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-ba1e9023-c78e-441e-97e0-7175c903a919\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.7343020439147949, -0.6444945335388184, -0.6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ba1e9023-c78e-441e-97e0-7175c903a919')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-ba1e9023-c78e-441e-97e0-7175c903a919 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-ba1e9023-c78e-441e-97e0-7175c903a919');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                            features\n",
       "0  [0.7343020439147949, -0.6444945335388184, -0.6..."
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run SQL query in Colab (no %sql)\n",
    "result_df = spark.sql(\"SELECT features FROM movie_on_movie WHERE id = 471\")\n",
    "\n",
    "# Display results\n",
    "result_df.show(truncate=False)\n",
    "result_df.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1cW0StfAJ1GQ",
    "outputId": "4a70bca3-5a12-41c6-c33f-12d8583519ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating| timestamp|\n",
      "+------+-------+------+----------+\n",
      "|  1551|    463|   2.0|1123107163|\n",
      "|  1756|    463|   4.0| 949288916|\n",
      "|  2862|    463|   3.5|1058144819|\n",
      "|  3331|    463|   2.0| 825638400|\n",
      "|  3408|    463|   2.0| 940862511|\n",
      "|  3809|    463|   4.0| 945013702|\n",
      "|  4028|    463|   4.0| 994087871|\n",
      "|  4903|    463|   2.0| 901636059|\n",
      "|  5070|    463|   4.0| 991852118|\n",
      "|  5533|    463|   4.0| 921624563|\n",
      "+------+-------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run SQL query using PySpark in Colab\n",
    "result_df = spark.sql(\"SELECT * FROM ratings WHERE movieId = 463 LIMIT 10\")\n",
    "\n",
    "# Show results\n",
    "result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "nNXw293UJ9XO"
   },
   "outputs": [],
   "source": [
    "brp = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\",seed=12345, bucketLength=1.0)\n",
    "#a.printSchema()\n",
    "#change features columns into dense vector\n",
    "to_vector = udf(lambda a: Vectors.dense(a), VectorUDT())\n",
    "data = a.select(\"id\", to_vector(\"features\").alias(\"features\"))\n",
    "#data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZYAHFJdGJ_Lg",
    "outputId": "75808eaa-31f0-4a16-e426-60d8c57008f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----------------------+\n",
      "|id |features   |hashes                 |\n",
      "+---+-----------+-----------------------+\n",
      "|0  |[1.0,1.0]  |[[-1.0], [0.0], [-1.0]]|\n",
      "|1  |[1.0,-1.0] |[[0.0], [0.0], [-1.0]] |\n",
      "|2  |[-1.0,-1.0]|[[0.0], [-1.0], [0.0]] |\n",
      "|3  |[-1.0,1.0] |[[-1.0], [-1.0], [0.0]]|\n",
      "+---+-----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.appName(\"BRPExample\").getOrCreate()\n",
    "\n",
    "# Example data\n",
    "data = spark.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 1.0])),\n",
    "    (1, Vectors.dense([1.0, -1.0])),\n",
    "    (2, Vectors.dense([-1.0, -1.0])),\n",
    "    (3, Vectors.dense([-1.0, 1.0]))\n",
    "], [\"id\", \"features\"])\n",
    "\n",
    "# Initialize BRP model\n",
    "brp = BucketedRandomProjectionLSH(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"hashes\",\n",
    "    bucketLength=2.0,\n",
    "    numHashTables=3\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "model = brp.fit(data)\n",
    "\n",
    "# Transform the data to generate hash values\n",
    "transformed = model.transform(data)\n",
    "\n",
    "# Show results\n",
    "transformed.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2n-75i85KLxv",
    "outputId": "6ffc42ab-45ac-41e0-d623-7009ce8bde3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------------------------------------------+----------------------+-------------------+\n",
      "|id |features                                                 |hashes                |distCol            |\n",
      "+---+---------------------------------------------------------+----------------------+-------------------+\n",
      "|1  |[-0.7,-1.1,-0.85,-0.65,-0.38,-0.9,-0.47,-0.16,0.36,-0.64]|[[0.0], [0.0], [-1.0]]|0.08196878989999681|\n",
      "|0  |[-0.8,-1.0,-0.9,-0.6,-0.4,-0.8,-0.5,-0.2,0.3,-0.6]       |[[0.0], [0.0], [-1.0]]|0.16355197558473514|\n",
      "|3  |[-0.9,-0.95,-0.8,-0.7,-0.45,-0.85,-0.48,-0.15,0.35,-0.63]|[[0.0], [0.0], [-1.0]]|0.20627610069436014|\n",
      "+---+---------------------------------------------------------+----------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.appName(\"ApproxNearestNeighborsExample\").getOrCreate()\n",
    "\n",
    "# Example dataset with 10-dimensional feature vectors\n",
    "data = spark.createDataFrame([\n",
    "    (0, Vectors.dense([-0.8, -1.0, -0.9, -0.6, -0.4, -0.8, -0.5, -0.2, 0.3, -0.6])),\n",
    "    (1, Vectors.dense([-0.7, -1.1, -0.85, -0.65, -0.38, -0.9, -0.47, -0.16, 0.36, -0.64])),\n",
    "    (2, Vectors.dense([0.5, 0.7, 0.6, 0.4, 0.8, 0.9, 1.0, 0.6, 0.4, 0.9])),\n",
    "    (3, Vectors.dense([-0.9, -0.95, -0.8, -0.7, -0.45, -0.85, -0.48, -0.15, 0.35, -0.63])),\n",
    "    (4, Vectors.dense([0.2, 0.1, 0.3, 0.4, 0.5, 0.4, 0.2, 0.1, 0.2, 0.3]))\n",
    "], [\"id\", \"features\"])\n",
    "\n",
    "# Create LSH model\n",
    "brp = BucketedRandomProjectionLSH(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"hashes\",\n",
    "    bucketLength=2.0,\n",
    "    numHashTables=3\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "model = brp.fit(data)\n",
    "\n",
    "# Define query vector\n",
    "query_vector = Vectors.dense([-0.73946416, -1.03179, -0.83905196, -0.6525196,\n",
    "                              -0.3816911, -0.88358724, -0.47698575, -0.15836999,\n",
    "                              0.36126232, -0.6475737])\n",
    "\n",
    "# Find top 6 nearest neighbors\n",
    "neighbors = model.approxNearestNeighbors(data, query_vector, 6)\n",
    "\n",
    "# Show results\n",
    "neighbors.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ylczNl2IKbh2",
    "outputId": "5efb7b23-f468-4217-c237-e79b3e17ee46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------------------------------+---------------------------+\n",
      "|movieId|title                                      |genres                     |\n",
      "+-------+-------------------------------------------+---------------------------+\n",
      "|1059   |William Shakespeare's Romeo + Juliet (1996)|Drama|Romance              |\n",
      "|3476   |Jacob's Ladder (1990)                      |Horror|Mystery             |\n",
      "|4346   |Bride of the Wind (2001)                   |Drama|Musical|Romance      |\n",
      "|6296   |Mighty Wind, A (2003)                      |Comedy|Musical             |\n",
      "|97057  |Kon-Tiki (2012)                            |Adventure|Documentary|Drama|\n",
      "+-------+-------------------------------------------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run SQL query in PySpark\n",
    "result_df = spark.sql(\"\"\"\n",
    "SELECT * FROM movies\n",
    "WHERE movieId IN (6296, 97057, 3476, 1059, 4346)\n",
    "\"\"\")\n",
    "\n",
    "# Show results\n",
    "result_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ec5IFLNEKl9e",
    "outputId": "c158d807-239e-4c33-f82a-f8810d197269"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------+\n",
      "|features                                                                                                                 |\n",
      "+-------------------------------------------------------------------------------------------------------------------------+\n",
      "|[-0.13110505, -0.19828054, -0.9705931, -0.060339108, -0.5328692, -1.100225, -0.6801733, 0.85767436, 1.5242684, 1.4728537]|\n",
      "+-------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make sure your DataFrame is registered as a temp view\n",
    "# movie_on_movie.createOrReplaceTempView(\"movie_on_movie\")\n",
    "\n",
    "# Run SQL query using PySpark\n",
    "result_df = spark.sql(\"\"\"\n",
    "SELECT features\n",
    "FROM movie_on_movie\n",
    "WHERE id = 463\n",
    "\"\"\")\n",
    "\n",
    "# Display the results\n",
    "result_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IXv_oBuhKwF6",
    "outputId": "688bb072-a3f5-4884-b808-b41131f42dd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------------------------------------------+------------------------+--------------------+\n",
      "|id |features                                                 |hashes                  |distCol             |\n",
      "+---+---------------------------------------------------------+------------------------+--------------------+\n",
      "|3  |[0.95,0.02,-0.33,0.37,0.21,-1.43,1.0,-0.07,0.42,-0.87]   |[[-1.0], [-1.0], [-1.0]]|0.02855520552451611 |\n",
      "|1  |[0.9,0.01,-0.34,0.38,0.19,-1.42,0.99,-0.065,0.43,-0.86]  |[[-1.0], [-1.0], [-1.0]]|0.041267376976824616|\n",
      "|0  |[1.0,0.0,-0.3,0.4,0.2,-1.4,1.0,-0.06,0.43,-0.86]         |[[-1.0], [-1.0], [-1.0]]|0.0815949474082027  |\n",
      "|4  |[0.2,0.1,0.3,0.4,0.5,0.4,0.2,0.1,0.2,0.3]                |[[-1.0], [-1.0], [0.0]] |2.540218265181665   |\n",
      "|2  |[-0.8,-1.0,-0.9,-0.6,-0.4,-0.8,-0.5,-0.2,0.3,-0.6]       |[[0.0], [0.0], [-1.0]]  |2.9018640896779693  |\n",
      "|5  |[-0.9,-0.95,-0.8,-0.7,-0.45,-0.85,-0.48,-0.15,0.35,-0.63]|[[0.0], [0.0], [-1.0]]  |2.947966817836074   |\n",
      "+---+---------------------------------------------------------+------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# 1️⃣ Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"ApproxNearestNeighborsExample\").getOrCreate()\n",
    "\n",
    "# 2️⃣ Example dataset\n",
    "data = spark.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.0, -0.3, 0.4, 0.2, -1.4, 1.0, -0.06, 0.43, -0.86])),\n",
    "    (1, Vectors.dense([0.9, 0.01, -0.34, 0.38, 0.19, -1.42, 0.99, -0.065, 0.43, -0.86])),\n",
    "    (2, Vectors.dense([-0.8, -1.0, -0.9, -0.6, -0.4, -0.8, -0.5, -0.2, 0.3, -0.6])),\n",
    "    (3, Vectors.dense([0.95, 0.02, -0.33, 0.37, 0.21, -1.43, 1.0, -0.07, 0.42, -0.87])),\n",
    "    (4, Vectors.dense([0.2, 0.1, 0.3, 0.4, 0.5, 0.4, 0.2, 0.1, 0.2, 0.3])),\n",
    "    (5, Vectors.dense([-0.9, -0.95, -0.8, -0.7, -0.45, -0.85, -0.48, -0.15, 0.35, -0.63]))\n",
    "], [\"id\", \"features\"])\n",
    "\n",
    "# 3️⃣ Create and fit LSH model\n",
    "brp = BucketedRandomProjectionLSH(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"hashes\",\n",
    "    bucketLength=2.0,\n",
    "    numHashTables=3\n",
    ")\n",
    "model = brp.fit(data)\n",
    "\n",
    "# 4️⃣ Define your query vector\n",
    "query_vector = Vectors.dense([\n",
    "    0.93929714, 0.015614069, -0.3408886, 0.3818301,\n",
    "    0.19762212, -1.4255825, 0.99496984, -0.065754086,\n",
    "    0.43202916, -0.8621043\n",
    "])\n",
    "\n",
    "# 5️⃣ Find top 6 nearest neighbors\n",
    "neighbors_df = model.approxNearestNeighbors(data, query_vector, 6)\n",
    "\n",
    "# 6️⃣ Show results\n",
    "neighbors_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TmyL28jvK2n2",
    "outputId": "a70ce566-10aa-42b9-d0f0-06e14cf3a4d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------------------+--------------------------------+\n",
      "|movieId|title                          |genres                          |\n",
      "+-------+-------------------------------+--------------------------------+\n",
      "|554    |Trial by Jury (1994)           |Crime|Drama|Thriller            |\n",
      "|5321   |Triumph of Love, The (2001)    |Comedy                          |\n",
      "|7224   |Boy with Green Hair, The (1948)|Children|Drama                  |\n",
      "|7276   |Hell's Kitchen (1998)          |Drama                           |\n",
      "|49007  |Arabesque (1966)               |Adventure|Drama|Romance|Thriller|\n",
      "+-------+-------------------------------+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make sure your movies DataFrame is registered as a temporary view first\n",
    "# (Run this once if you haven’t already)\n",
    "movies_df.createOrReplaceTempView(\"movies\")\n",
    "\n",
    "# Now run the SQL query\n",
    "result_df = spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM movies\n",
    "WHERE movieId IN (5321, 49007, 554, 7276, 7224)\n",
    "\"\"\")\n",
    "\n",
    "# Display the results\n",
    "result_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (833241955.py, line 14)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m.config(\"spark.jars\", local_jar_path) \\  # <-- IMPORTANT: 'spark.jars' (no 'packages')\u001b[39m\n                                                                                          \n^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "# Configure SparkSession to include the MongoDB Spark connector\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Using the provided MongoDB connection string\n",
    "mongo_uri = \"mongodb+srv://vishureddy1224_db_user:Xdjcy7KrT8ZT8aQv@cluster0.0mb3m83.mongodb.net/?appName=Cluster0\"\n",
    "\n",
    "# --- THIS IS THE FIX ---\n",
    "# Point to the JAR file you just downloaded.\n",
    "# (Update this path if you placed the JAR file somewhere else)\n",
    "local_jar_path = \"mongo-spark-connector_2.12-3.0.1.jar\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"movie analysis\") \\\n",
    "    .config(\"spark.jars\", local_jar_path) \\\n",
    "    .config(\"spark.mongodb.input.uri\", mongo_uri + \"/bda_project.user_ratings\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", mongo_uri + \"/bda_project.user_recommendations\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark with LOCAL Mongo connector configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read ratings from MongoDB into a Spark DataFrame\n",
    "ratings_df = spark.read.format(\"mongo\")\\\n",
    "    .option(\"uri\", mongo_uri + \"/bda_project.user_ratings?retryWrites=true&w=majority\")\\\n",
    "    .load()\n",
    "ratings_df.createOrReplaceTempView(\"movie_ratings\")\n",
    "spark.sql(\"SELECT * FROM movie_ratings LIMIT 5\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best_model (PySpark ALS model) to local disk so the streaming job can load it\n",
    "# This assumes 'best_model' is available in the notebook scope (e.g. from CV tuning)\n",
    "model_save_path = \"my_als_model\"\n",
    "try:\n",
    "    best_model.write().overwrite().save(model_save_path)\n",
    "    print(\"Saved best_model to\", model_save_path)\n",
    "except Exception as e:\n",
    "    print(\"Error saving model:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the 'user_recs' DataFrame to MongoDB 'bda_project.user_recommendations' (overwrite)\n",
    "# 'user_recs' should be a DataFrame with schema: userId, recommendations (array of structs with movieId and rating)\n",
    "out_uri = mongo_uri + \"/bda_project.user_recommendations?retryWrites=true&w=majority\"\n",
    "try:\n",
    "    user_recs.write.format(\"mongo\").mode(\"overwrite\").option(\"uri\", out_uri).save()\n",
    "    print(\"Wrote user_recs to MongoDB collection bda_project.user_recommendations\")\n",
    "except Exception as e:\n",
    "    print(\"Error writing recommendations to MongoDB:\", e)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
